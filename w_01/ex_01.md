# Exercise notebook #1: *Introduction to Probability*

### Stochastic Modeling and Simulation $\in$ DSSC @ University of Trieste (Spring 2021)

### Submitted by: [Emanuele Ballarin](mailto:emanuele@ballarin.cc)



## Exercise 1

Let $X$ be a random variable distributed uniformly along the interval $[a,b]$, i.e. $X \sim \mathcal{U}(a,b)$. Show that:

-   $\mathbb{E}[X] = \frac{a+b}{2}$;
-   $Var(X) = \frac{(b-a)^2}{12}$.

### Solution:

The *r.v.* $X$ is a continuous *r.v.* (being it a *uniform r.v.* along an interval). As such, its expected value $\mathbb{E}[X]$ and variance $Var(X)$ are defined as follows – being $p(x)$ the *p.d.f.* of $X$, defined over all possible outcomes $X=x \in (-\infty, +\infty)$.

-   $\mathbb{E}[X] = \int_{-\infty}^{+\infty}{x\ p(x)\ dx}$
-   $Var(X) = \int_{-\infty}^{+\infty}{(x -\mathbb{E}[X])^2 \ p(x)\ dx}$

The latter can be reworked by exploiting linearity of integral operators over fixed domains and invariance of $\mathbb{E}[X]$ *w.r.t.* sampling (i.e. specific realizations $x$ of $X$), thus leading to:

-   $Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2$, with $\mathbb{E}[f(X)]$ generally denoting $\int_{-\infty}^{+\infty}{f(x)\ p(x)\ dx}$.

In the specific case of a uniform distribution along an interval $X \sim \mathcal{U}(a,b)$, the *p.d.f.* $p(x)$ can be built by imposing both:

-   The uniformity constraint $p(x) = \propto \tilde{p}(x) \ \wedge\ \tilde{p}(x) = \tilde{p}(x')\ \forall x,x' \in [a,b]\ \wedge\ \tilde{p}(x'') = 0 \ \forall x'' \in (-\infty, +\infty) \setminus [a,b]$;
-   The normalization constraint $\int_{-\infty}^{+\infty}{p(x)\ dx} \ =\ 1$,

which univocally determine the (normalized) *p.d.f.* $p(x) = \frac{1}{b-a}$.

From previous definitions, for the expected value of  $X$:

-   $\mathbb{E}[X] = \int_{-\infty}^{+\infty}{x\ p(x)\ dx} \ = \ \frac{1}{b-a}\int_{a}^{b}{x\ dx} = \frac{b^2 - a^2}{2(b-a)} = \frac{(b - a)(b+a)}{2(b-a)} = \frac{a+b}{2}$.

For the variance of $X$, we first compute:

-   $\mathbb{E}[X^2] = \int_{-\infty}^{+\infty}{x^2\ p(x)\ dx} \ = \ \frac{1}{b-a}\int_{a}^{b}{x^2\ dx} = \frac{b^3 - a^3}{3(b-a)} = \frac{(b-a)(a^2+b^2+ab)}{3(b-a)}= \frac{a^2+b^2+ab}{3} = \frac{4a^2+4b^2+4ab}{12}$.
-   $\mathbb{E}[X]^2 = \frac{(a+b)^2}{4}= \frac{a^2+b^2+2ab}{4} = \frac{3a^2+3b^2+6ab}{12}$

Then, simply by differencing:

-   $Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \frac{a^2+b^2-2ab}{12} = \frac{(b-a)^2}{12}$

---

## Exercise 2

Suppose that you have 8 cards. 5 of them are green ($G$) and 3 are yellow ($Y$). The five $G$ cards are numbered 1, 2, 3, 4, 5. The three $Y$ cards are numbered 1, 2, 3. The cards are well shuffled. You randomly draw 1 card. Denote with $E$ the event of drawing an even-numbered card, and compute.

1.  $P(G)$
2.  $P(G|E)$
3.  $P(G \wedge E)$
4.  $P(G \vee E)$
5.  Are $G$ and $E$ mutually exclusive?

### Solution 1: *Counting frenzy*

The entire problem *could* be solved with no more than counting and the computation of ratios. Indeed, being the event set (the set of all possible outcomes we could witness in repeatedly *experiencing* the situation described until a fixed point is reached) discrete and finite, we can fall-back to the Laplacian *common-sense based* approach to probability.

Specifically, we are interested in the event of drawing 1 card from a finite deck composed of cards satisfying the properties described before. The set of all possible events we can witness (and therefore think of) has as elements each and all the events of *extracting one specific card* of the deck, among all of them. Its cardinality is easily computed as $|\Omega| = 8$.

Since the two properties $G$ (whose opposite $¬G = Y$) and $E$ (whose opposite $¬E = O$) are sufficient to univocally identify the cards of the deck (by replacing the number of each given-colour card with its *even*/*odd* specification, eventually indexed to preserve distinguishability), so are the events of the event set.
I.e.: $\Omega := \{\text{1G, 2G, 3G, 4G, 5G, 1Y, 2Y, 3Y}\} = {\{\text{OG, EG, OG', EG', OG'', OY, EY, OY'}\}}$. In such description, the juxtaposition of property specifiers ($G$, $Y$, $E$, $O$) denotes that both properties are satisfied; meaning that e.g. $OY \equiv (O \wedge Y)$ and the presence of a given index (or lack thereof) specifies that the event is distinguishable from all others identified by the same pair of properties (which they satisfy) but a different index.

By acknowledging that $p(\text{property}) = p(e\ \text{s.t.}\ \text{property}(e)\ \text{is true}) = \frac{\#\ e \in \Omega\ \text{s.t.}\ \text{property}(e)\ \text{is true}}{\#\ e \in \Omega} = \frac{|\Omega_{\text{property}}|}{|\Omega|}$, points (i), (iii) and (iv) are directly solvable by counting the specific events satisfying the proposed property (or their conjunction / disjunction) and dividing the result by $|\Omega|$.

Point (ii) can be solved by acknowledging that, in the Laplacian approach to probability, the conditioning set contains the truth condition that defines the (new) event set to consider. In our case, conditioning on $E$ is equivalent to redefining the event set as $\Omega_E$ and then computing $p(\text{property}| E) = \frac{|\Omega_{\text{property}|E}|}{|\Omega_E|}$.

Point (v) is solved by noticing that $|\Omega_{G \wedge E}| >0$ (meaning that the events are not mutually exclusive).

### Solution 2: *Three numbers suffice*

At the totally opposite end of the spectrum there is the approach consisting in counting the *least possible* in order to solve all the possible problems involving the situation just described. What follows could be even performed without knowing which specific questions we will be going to answer afterwards. The counting procedure is the same as before, but it is performed just *w.r.t.* the four properties $G\wedge E$, $G\wedge O$, $Y\wedge E$, $Y\wedge O$, thus leading to the corresponding probabilities.

The probability of one (of the four) property could even be computed from the remaining three, via normalization, i.e. $p(\Omega)=1$. Such three numbers (to be obtained by counting) fully describe the outcome-generating process *w.r.t.* the properties we are interested in.

The process just described is indeed the construction of a probability mass function of the event *draw of a card from the deck* according to the two coordinates *even or odd* and *green or yellow*.

Probabilities involving just one among all (two) properties can be computed by marginalizing (i.e. summing over all possible values of) the other variable(s); unions of events can be reduced to set-theoretical operations involving negations of single properties and intersections – or just by exploiting the weak version of disjointness axiom (even pre-dating Kolmogorov’s) *a.t.w.* $p(¬A) = 1-p(A)$.

Conditioning is performed via quotient of probabilities, i.e. $p(A|B) = \frac{p(A\wedge B)}{p(B)}$.

Point (v) is addressed by noticing that $p(G\wedge E)>0$.

### The actual solution: putting numbers in

Either way they are computed (or in a mix of them):

1.  $P(G)= \frac{5}{3+5}=\frac{5}{8}$. E.g. via direct plug-in of numbers given by the text.
2.  $P(G|E) = \frac{P(G\wedge E)}{P(E)} = \frac{2/8}{3/8} = \frac{2}{3}$. E.g. via definition of conditional probability. Counting is used to determine:
3.  $P(G\wedge E) = \frac{2}{8} = \frac{1}{4}$.
4.  $P(G \vee E) = 1-P(Y\wedge O) = 1-\frac{2}{8} = \frac{3}{4}$. E.g. $P(Y\wedge O)$ via counting.
5.  They are not.

*P.S.: Was the exercise **only** about this third part?* :speak_no_evil:

---

## Exercise 3

Consider a population containing $n$ individuals, $m$ of which are infected. You invite 2 people over for dinner. What is the probability that at least one of the two guests are infected?

### Solution:

The problem can be easily solved when mapped to an *urns and balls sampling problem*, where $n$ balls – $m$ red and $(n-m)$ black – are put inside an urn. The probability $p_?$ to be determined is that of extracting, twice and without replacement, at least one red ball. The order of extraction is irrelevant.

The most straightforward way to attack the problem is to consider the complementary event (i.e. extracting – in the same setting as before – exactly two black balls), and then compute $p_? = 1-p_{BB}$. This way, both the problem of draw order and that of asymmetry between first and second draw due to lack of replacement are solved. *For free!*

We can now compute $p_{BB} = \frac{n-m}{n}\frac{n-m-1}{n-1}$ and the requested $p_? = 1-\frac{n-m}{n}\frac{n-m-1}{n-1}$.

---

## Exercise 4

Let $X$ (and $Y$, when needed) be a discrete random variable; let $a$ and $b$ arbitrary given constants. Prove that:

1.  $\mathbb{E}[aX + b] = a\mathbb{E}[X]+b$;
2.  $Var(aX + b) = a^2Var(X)$;
3.  $\mathbb{E}[X + Y] = \mathbb{E}[X]+\mathbb{E}[Y]$;
4.  $Var(X + Y) = Var(X)+ Var(Y)$ if $X$ and $Y$ are independent.
5.  How can you express $Cov(X, Y)$ in case they are not *i.r.v.*s?

### Solution:

Being $X$ a discrete *r.v.*, we acknowledge the following definitions for $E[f(X)]$ and $Var(f(X))$, with $f$ any function preserving the well-posedness of the problem. The same applies for $Y$.

-   $E[f(X)] = \sum_{\ i \in \Omega \ } f(x_i)\ p(x_i)$
-   $Var(f(X)) = \sum_{\ i \in \Omega \ } (f(x_i) - E[f(X)])^2\ p(x_i)$

with $\Omega$ being the event set for $X$, $i \in \Omega$ the index of any possible realization of $X$, $p(x)$ the (normalized) probability mass function for $X$ and $f$ including the identity function. Let $p(x_i, y_i)$ be the joint (normalized) probability mass function for $X$ and $Y$.

1.  $\mathbb{E}[aX + b] = \sum_{i \in \Omega} p(x_i)\ (ax_i+b) = \sum_{i \in \Omega} p(x_i)\ (ax_i) + \sum_{i \in \Omega} p(x_i)\ (b) =\\= a\sum_{i \in \Omega} p(x_i)\ (x_i) + b\sum_{i \in \Omega} p(x_i) = a\mathbb{E}[X]+b$

    

2.  $Var(aX + b) = \sum_{i \in \Omega} p(x_i)\ ((ax_i+b) - \mathbb{E}[aX + b])^2 = \sum_{i \in \Omega} p(x_i)\ (ax_i+b - a\mathbb{E}[X]+b)^2 =\\= \sum_{i \in \Omega} p(x_i)\ (ax_i - a\mathbb{E}[X])^2 = a^2\sum_{i \in \Omega} p(x_i)\ (x_i - \mathbb{E}[X])^2 = a^2 Var(X)$

    

3.  $\mathbb{E}[X + Y] = \sum_{(i,j) \in \Omega_X\otimes \Omega_Y} p(x_i, y_j)\ (x_i+y_j) = \sum_{i \in \Omega_X, j \in \Omega_Y} x_i p(x_i, y_j) + y_jp(x_i, y_j) =\\= \sum_{i \in \Omega_X} \sum_{j \in \Omega_Y} x_i p(x_i, y_j) + \sum_{j \in \Omega_Y} \sum_{i \in \Omega_X}y_jp(x_i, y_j) =\\= \sum_{i \in \Omega_X} x_i \sum_{j \in \Omega_Y} p(x_i, y_j) + \sum_{j \in \Omega_Y} y_j \sum_{i \in \Omega_X}p(x_i, y_j) =\\= \sum_{i \in \Omega_X} x_i p(x_i) + \sum_{j \in \Omega_Y} y_j p(y_j) = \mathbb{E}[X]+\mathbb{E}[Y]$ 

    

4.  In the case $X$ and $Y$ are independent, it is true that $p(x_i, y_j) = p(x_i)p(y_j)$.

    $Var(X + Y) = \sum_{(i,j) \in \Omega_X\otimes \Omega_Y} p(x_i, y_j)\ ((x_i - \mathbb{E}[X])^2+(y_j - \mathbb{E}[Y])^2) =\\= \sum_{i \in \Omega_X, j \in \Omega_Y} p(x_i, y_j)\ (x_i - \mathbb{E}[X])^2+\sum_{i \in \Omega_X, j \in \Omega_Y} p(x_i, y_j)(y_j - \mathbb{E}[Y])^2 =\\= \sum_{i \in \Omega_X} p(x_i)(x_i - \mathbb{E}[X])^2 \sum_{j \in \Omega_Y} p(y_j) + \sum_{j \in \Omega_Y} p(y_j) (y_j - \mathbb{E}[Y])^2 \sum_{i \in \Omega_X}p(x_i) =\\= \sum_{i \in \Omega_X} p(x_i)(x_i - \mathbb{E}[X])^2 + \sum_{j \in \Omega_Y} p(y_j) (y_j - \mathbb{E}[Y])^2= Var(X)+ Var(Y)$

    

5.  Since there are many different ways to manipulate and rearrange the *covariance* expression for two non-*i.r.v.*, we assume the request refers to the *canonical* expansion of the *covariance* as a function of the *variance of the sum* and the *variance of the single r.v.s*.

    We start off by recalling the definition of covariance $Cov(X, Y) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]$, and the direct consequence that $Var(X) = Cov(X,X)$, which also holds true for $Y$.

    Then, by exploiting the linearity of the $\mathbb{E}$ operator (see previous answers), we compute:

    $Var(X + Y) = Cov(X+Y, X+Y) = Cov(X,X) + Cov(Y,Y) + 2Cov(X,Y)$.

    It directly follows that, in the general case: $Cov(X,Y) = \frac{1}{2} (Var(X + Y) - (Var(X)+ Var(Y)))$.

    With respect to this last result, we notice that the term $Var(X)+ Var(Y)$ is indeed the variance of $X+Y$ in the hypothetical case $X$ and $Y$ were independent.

---

## Exercise 5

Let $V = \frac{1}{3}(U_1 + U_2 + U_3)$ be a random variable defined as the average of three *i.i.d.* and uniformly distributed random variables $U_i \sim U(0,1)$. Determine the expected value and the variance of $V$.

### Solution:

In the following, we will use (without re-demonstrating their validity):

-   The properties (especially: *mean* and *variance*) of a *Uniform* distribution defined over an interval $[a,b]$; in our case $a=0$ and $b=1$. See *Exercise 1*.
-   The linear transformation properties of expectations, and the transformation they induce on *mean* and *variance* of linear combinations among (eventually independent) random variables. Such properties have been proven in *Exercise 4* for the case of discrete *r.v.s*, but they can be likewise proven – by following the exact same procedure with continuous integral operators, instead of discrete (summation) – in the case of arbitrary distributions which can be represented as function-form *p.d.f.s* (which is the case of the *Uniform*. Special care must eventually be taken in the case of *non-function-form distributions* (e.g. *Dirac’s* $\delta$) and limit-process distributions.

Specifically, in the case of $U_1$, $U_2$, $U_3$ *i.i.d. uniform r.v.s* over the $[0,1]$ interval, we have that:

-   $\mathbb{E}[U_1] = \mathbb{E}[U_2] = \mathbb{E}[U_3] = \frac{1}{2} = M$
-   $Var(U_1) = Var(U_2) = Var(U_3) = \frac{1}{12} = V$
-   $\mathbb{E}[V] = \mathbb{E}[\frac{U_1 + U_2 + U_3}{3}] = \frac{3M}{3} = M = \frac{1}{2}$
-   $Var(V) = Var(\frac{U_1+U_2+U_3}{3}) = \frac{1}{9}(3V) = \frac{V}{3} = \frac{1}{32}$

As we can notice, the mean of $V$ is independent of the number of variables summed (in this case: 3) and the variance is inversely-proportional to such number. This is a simple example of *concentration property* of the mean of *i.i.d.* *r.v.s* (that also works in the case of *weakly-correlated* *r.v.s*).

---

## Exercise 6

${}^{14}C$ is a radioactive element with a half-life of about $5730$ years. ${}^{14}C$ decays following an exponential distribution with a rate of $0.000121$. We start with some ${}^{14}C$ and we are interested in the time (in years) if takes to decay.

1.  Find the percentage of ${}^{14}C$ lasting less than $5730$ years;
2.  Find the percentage of ${}^{14}C$ lasting more than $10000$ years;
3.  How many years are necessary for $30\%$ of ${}^{14}C$ to decay?

### Solution:

In the context of *radionuclide decay*, the *decay* of a single nucleus is *memoryless*, and the occurrence of a given number  of nuclei decaying is *densely uniform* in time, given a sufficiently large sample of nuclei. This leads to the *exponential-distribution* (function of $t$) modeling of the probability of *one* nucleus decaying at exactly time $t$ or, equivalently, of the expected fraction of nuclei decaying at exactly time $t$.

Formally: $p(t) = \lambda e^{-\lambda t}$, with the parameter $\lambda$ called the *rate* of the disstribution (or of *the decay*, in our specific case).

From the *p.d.f.* above, we can easily compute (via integration in the $[0,t]$ interval) the *c.d.f.* being $1-e^{-\lambda t} = C(t)$.
All remaining solutions can be computed directly via differencing $C$ computed in different $t$s (or its inverse problem).

1.  We directly compute $C(t=5730) \cong 0.5 = 50\%$. Indeed, it is called *half*-life for a reason. :upside_down_face:
2.  We directly compute $C(t \rightarrow +\infty) - C(t=10000) = 1-C(t=10000) \cong 0.3 = 30\%$
3.  We solve *w.r.t.* $t^{*}$ for $0.3 = C(t^{*})$ , obtaining: $t^{*} \cong 2948$ years.

---

## Exercise 7

1.  The patient recovery time from a particular surgical procedure is normally distributed with a mean of $5.3$ days and a standard deviation of $2.1$ days.
    -   What is the median recovery time?
    -   What is the $z$-score for a patient who takes 10 days to recover?
2.  The length of time to find a parking space at 9 A.M. follows a normal distribution with a mean of 5 minutes and a standard deviation of 2 minutes.
    -   Find the probability that it takes at least 8 minutes to find a parking space.
3.  Since times cannot assume negative values, compute the same quantities of problems (1) and (2) using a normal distribution truncated at zero.

### Solution:

1.  The distribution of *patient recovery time* is a $\mathcal{N}(\mu=5.3; \sigma=2.1)$.

    -   The *mean* of a *Normal distribution* coincides with its *mode* and *median*, implying that the *median recovery time* is indeed $\mu=5.3$ days.
    -   We compute, by definition, $z = \frac{x-\mu}{\sigma}$, which in our case ($x=10$) becomes $\frac{10-5.3}{2.1} \cong 2.24 $.

2.  The distribution of *(exact) time it takes to find a free parking lot* is a $\mathcal{N}(\mu=5; \sigma=2)$. We call $\mathcal{C}(\mu=5; \sigma=2)$ the associated *cdf*, which can be expressed in terms of the *error function* $\text{erf}(\frac{x-\mu}{\sqrt{2}\sigma})$ and computed via table lookup (or *empirical sampling*).

    -   We just compute $1-\mathcal{C(\mu=5; \sigma=2)[x=10]}$, obtaining a probability $P\cong 0.0062$.

3.  In order to compute the requested values, one has to build first the *(normalized) truncated Normal density function*, in the manner outlined as follows. We assume, for the widest generality possible, a two-sided truncation in the $[a, b]$ interval and call $p(x)$ the *(normalized) Normal density function*.

    -   Compute the normalization constant: $\Omega = \int_a^b p(x) dx$.
    -   Define the *(normalized) truncated Normal density function* $p_{trunc}(x) = \begin{cases} 0 \text{ for } x \in(-\infty, a],\\ \frac{p(x)}{\Omega} \text{ for } x \in(a,b) \\ 0 \text{ for } x \in[b, +\infty) \end{cases}$.
    -   Integrate whichever $f(x)\ p_{trunc}(x)$ needed to compute the requested expectations $\mathbb{E}(f(x))$.

    Since integration of such functions may be cumbersome to perform analytically, we resort to *numerical-tabular* methods, via the use of the [`SciPy`](https://www.scipy.org/) package.

    In what follows, we **redefine** the $p(x) \leftarrow p_{trunc}(x)$ and the associated *cdf* $\mathcal{C}(\mu, \sigma) \leftarrow \mathcal{C}_{trunc}(\mu, \sigma)$.

    1.  Median: $\cong 5.32$; $z \cong 2.27$
    2.  Probability: $P\cong 0.0062$ (the difference with the previous is indeed negligible at this level of significance).